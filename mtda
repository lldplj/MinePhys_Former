import torch
import torch.nn as nn


class MTDA_Attention(nn.Module):
    def __init__(self, d_model, n_heads, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, d_model)
        self.v_proj = nn.Linear(d_model, d_model)
        self.base_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)
        self.diff_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)
        self.snr_estimator = nn.Sequential(
            nn.Conv1d(d_model, d_model // 2, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(d_model // 2, 1),
            nn.Sigmoid()
        )
        self.out_proj = nn.Linear(d_model, d_model)
        self.norm = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, attn_mask=None):
        B, L, D = x.shape
        Q = self.q_proj(x)
        K = self.k_proj(x)
        V = self.v_proj(x)
        normal_out, _ = self.base_attn(Q, K, V, attn_mask=attn_mask)
        # [B, L, D] -> [B, L-1, D]
        x_diff = x[:, 1:, :] - x[:, :-1, :]
        x_diff = torch.cat([x_diff[:, 0:1, :], x_diff], dim=1)
        Q_diff = self.q_proj(x_diff)
        K_diff = self.k_proj(x_diff)
        V_diff = self.v_proj(x_diff)
        diff_out, _ = self.diff_attn(Q_diff, K_diff, V_diff, attn_mask=attn_mask)
        snr_weight = self.snr_estimator(x.transpose(1, 2)).unsqueeze(1)
        fused_out = normal_out + snr_weight * diff_out
        fused_out = self.out_proj(fused_out)
        output = self.norm(x + self.dropout(fused_out))
        return output
